{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:40px;\"><center>Exercise I:<br> Training of simple MLP models\n",
    "</center></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Short summary\n",
    "In this exercise you will: \n",
    "\n",
    "* train MLPs for simple classification and regression problems.\n",
    "* learn how hyper-parameters such as learning rate, batch size and number of epochs affect training.\n",
    "\n",
    "You should write the report of the exercise within this notebook. The details of how to do that can be found below in section \"Writing the report\".\n",
    "\n",
    "**Deadline for submitting the report: See Canvas assignment.**\n",
    "\n",
    "## The data\n",
    "We will use two synthetic different data sets in this exercise\n",
    "\n",
    "### syn2\n",
    "The *syn2* dataset represents a binary classification problem. The input data is 2D which allows for an easy visual inspection of the different classes and the decision boundary implemented by the network. The dataset is generated using random numbers each time you run the cell. This means that each time you generate the data it will be slightly different. You can control this by having a fixed *seed* to the random number generator. The cell \"PlotData\" will plot the *syn2* dataset.\n",
    "\n",
    "### regr1\n",
    "The *regr1* dataset represents a regression problem. It has one input and one target variable. It a cosinus function, with the possibility to add some noise and dampening on the output. Again see the cell \"PlotData\" to look at the dataset.\n",
    "\n",
    "## The exercises\n",
    "There are 8 questions in this exercise. These 8 questions can be found in three different cells below (see section \"The Different Cells\"). The first 6 questions will use the *regr1* dataset and questions 7-8 will use *syn2*.\n",
    "\n",
    "## The different 'Cells'\n",
    "This notebook contains several cells with python code, together with the markdown cells (like this one) with only text. Each of the cells with python code has a \"header\" markdown cell with information about the code. The table below provides a short overview of the code cells. \n",
    "\n",
    "| #  |  CellName | CellType | Comment |\n",
    "| :--- | :-------- | :-------- | :------- |\n",
    "| 1 | Init | Needed | Sets up the environment|\n",
    "| 2 | MLP | Needed | Defines the MLP model |\n",
    "| 3 | Data | Needed | Defines the functions to generate the artificial datasets |\n",
    "| 4 | PlotData | Information | Plots the 2D classification datasets |\n",
    "| 5 | Statistics | Needed | Defines the functions that calculates various performance measures |\n",
    "| 6 | Boundary | Needed | Function that can show 2D classification boundaries | \n",
    "| 7 | CustomEpochStatistics | Needed | Function to compute (better) per epoch statistics | \n",
    "| 8 | Ex1 | Exercise | For question 1-4 |\n",
    "| 9 | Ex2 | Exercise | For question 5-6 |\n",
    "| 10 | Ex3 | Exercise | For question 7-8 |\n",
    "\n",
    "To start with the exercise you need to run all cells with the celltype \"Needed\". The very first time we suggest that you enter each of the needed cells, read the cell instruction and run the cell. It is important that you do this in the correct order, starting from the top and work your way down the cells. Later, when you have started to work with the notebook it may be easier to use the command \"Run All\" or \"Run all above\" found in the \"Cell\" dropdown menu.\n",
    "\n",
    "## Writing the report\n",
    "The report should be written within this notebook. We have prepared the last cell in this notebook for you where you should write the report. The report should contain 4 parts:\n",
    "\n",
    "* Name:\n",
    "* Introduction: A **few** sentences where you give a small introduction about the content and purpose of the lab.\n",
    "* Answers to questions: For each of the questions provide an answer. It can be short answers or a longer ones depending on the nature of the questions, but try to be effective in your writing.\n",
    "* Conclusion: Summarize your findings in a few sentences.\n",
    "\n",
    "It is important that you write the report in this last cell and **not** after each question! \n",
    "\n",
    "## Last but not least\n",
    "Have fun!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: Init (#1)\n",
    "### CellType: Needed\n",
    "### Cell instruction: Initializing the libraries\n",
    "\n",
    "In the cell below, we will import needed libraries. \n",
    "\n",
    "Run the cell by entering into the cell and press \"CTRL Enter\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import metrics, regularizers, optimizers\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random as rn\n",
    "import scipy as sp\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import confusion_matrix, log_loss, classification_report\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: MLP (#2)\n",
    "### CellType: Needed\n",
    "### Cell instruction: Defining the MLP model\n",
    "\n",
    "This cell defines the MLP model. There are a number of parameters that is needed to \n",
    "define a model. Here is a list of them: **Note:** They can all be specified when you call\n",
    "this function in later cells. The ones specified in this cell are the default values.\n",
    "\n",
    "* inp_dim: the input dimension (integer)\n",
    "\n",
    "* n_nod: size of the network, eg [5] for a one hidden layer with 5 nodes and [5,3] for a two layer network with 5 and 3 hidden nodes each.\n",
    "\n",
    "* act_fun: the activation function. Most common are\n",
    "    * 'relu'\n",
    "    * 'tanh'\n",
    "        \n",
    "* out_act_fun: the activation function for the output nodes. Most common are\n",
    "    * 'linear'\n",
    "    * 'sigmoid'\n",
    "    * 'softmax'\n",
    "    \n",
    "* opt_method: The error minimization method. Common choices\n",
    "    * 'SGD'\n",
    "    * 'Adam'\n",
    "    \n",
    "* cost_fun: The error function used during training. There are three common ones\n",
    "    * 'mean_squared_error'\n",
    "    * 'binary_crossentropy'\n",
    "    * 'categorical_crossentropy'\n",
    "\n",
    "* lr_rate: The learning rate.\n",
    "\n",
    "* metric: The metric to use besides the loss. Common values\n",
    "    * accuracy\n",
    "    * mse\n",
    "\n",
    "Run the cell by entering into the cell and press \"CTRL Enter\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(inp_dim,\n",
    "            n_nod,\n",
    "            act_fun = 'tanh',\n",
    "            out_act_fun = 'linear',\n",
    "            opt_method = 'SGD',\n",
    "            cost_fun = 'mse',\n",
    "            lr_rate = 0.01,\n",
    "            metric = 'mse',\n",
    "            num_out = None):\n",
    "    \n",
    "    lays = [inp_dim] + n_nod\n",
    "    \n",
    "    main_input = Input(shape=(inp_dim,), dtype='float32', name='main_input')\n",
    "    \n",
    "    X = main_input\n",
    "    for i, nod in enumerate(n_nod):\n",
    "        X = Dense(nod, \n",
    "                  activation = act_fun)(X)\n",
    "        \n",
    "    output = Dense(1, activation = out_act_fun )(X)\n",
    "    \n",
    "    method = getattr(optimizers, opt_method)\n",
    "    \n",
    "    model =  Model(inputs=[main_input], outputs=[output])\n",
    "    model.compile(optimizer = method(learning_rate = lr_rate),\n",
    "                  loss = cost_fun,\n",
    "                  metrics=[metric])       \n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: Data (#3)\n",
    "### CellType: Needed\n",
    "### Cell instruction: Defining synthetic data sets\n",
    "\n",
    "This cell defines the two synthetic datasets. The last function is used for standardization of the data. \n",
    "\n",
    "Run the cell by entering into the cell and press \"CTRL Enter\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syn2(N):\n",
    "    \n",
    "    global seed\n",
    "     \n",
    "    x = np.empty(shape=(N,2), dtype = np.float32)  \n",
    "    d = np.empty(shape=(N,), dtype = np.float32) \n",
    "    N1 = int(N/2)\n",
    "\n",
    "    # Positive samples\n",
    "    x[:N1,:] = 0.8 + np.random.normal(loc=.0, scale=1., size=(N1,2))\n",
    "\n",
    "    # Negative samples \n",
    "    x[N1:,:] = -.8 + np.random.normal(loc=.0, scale=1., size=(N-N1,2))\n",
    "    \n",
    "    # Target\n",
    "    d[:N1] = np.ones(shape=(N1,))\n",
    "    d[N1:] = np.zeros(shape=(N-N1,))\n",
    "\n",
    "    return x,d\n",
    "\n",
    "\n",
    "def regr1(N, len = 2, damp=0, v=0):\n",
    "\n",
    "    global seed\n",
    "\n",
    "    dx = 2*len*np.pi /(N-1)\n",
    "    x = np.empty(N, dtype = np.float32) \n",
    "    for i in range(N):\n",
    "        x[i] = i*dx\n",
    "    \n",
    "    noise =  lambda  n : np.random.normal(0,1,n)\n",
    "    if damp == 1:\n",
    "        d = np.cos(x)*np.exp(-x*0.05)\n",
    "    else:\n",
    "        d = np.cos(x)\n",
    "    \n",
    "    std_signal = np.std(d)\n",
    "    d = d + v * std_signal * noise(N)\n",
    "        \n",
    "    return x, d\n",
    "\n",
    "\n",
    "def standard(x):\n",
    "    return np.mean(x, axis=0) , np.std(x, axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: PlotData (#4)\n",
    "### CellType: Information\n",
    "### Cell instruction: Plotting the data\n",
    "\n",
    "Here we just generate 100 cases for *syn2* and the *regr1* dataset and plot them. \n",
    "\n",
    "Run the cell by entering into the cell and press \"CTRL Enter\". \n",
    "\n",
    "**Note!** This cell is not needed for the actual exercises, it is just to visualize the two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# seed = 0 means random, seed > 0 means fixed\n",
    "seed = 0\n",
    "np.random.seed(seed) if seed else None\n",
    "\n",
    "x,d = syn2(100)\n",
    "plt.figure(1)\n",
    "plt.scatter(x[:,0],x[:,1], c=d)\n",
    "\n",
    "# Regression, one period, no noise\n",
    "x,d = regr1(100, 2, 0, 0)\n",
    "plt.figure(2)\n",
    "plt.scatter(x,d)\n",
    "\n",
    "# Regression, 1.5 period, exponential damping, some noise\n",
    "x,d = regr1(100, 3, 1, 0.2)\n",
    "plt.figure(3)\n",
    "plt.scatter(x,d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: Statistics (#5)\n",
    "### CellType: Needed\n",
    "### Cell instruction: Present result for both classification and regression problems\n",
    "\n",
    "This cell defines two functions that we are going to call using a trained model to calculate both error and performance measures. \n",
    "\n",
    "Run the cell by entering into the cell and press \"CTRL Enter\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_class(x = None, d = None, label='training', model = None):\n",
    "    \"\"\"\n",
    "    input :  \n",
    "             x = input\n",
    "             d = target\n",
    "             label = \"Provided text string\"\n",
    "             model = the model\n",
    "             \n",
    "    output : \n",
    "             accuracy = fraction of correctly classified cases\n",
    "             loss = typically the cross-entropy error\n",
    "    \"\"\"\n",
    "    \n",
    "    def binary(y1):\n",
    "        y1[y1>.5] = 1.\n",
    "        y1[y1<= .5] = 0.        \n",
    "        return y1\n",
    "\n",
    "    d_pr = model.predict(x, batch_size = x.shape[0], verbose=0).reshape(d.shape)\n",
    "                \n",
    "    nof_p, tp, nof_n, tn = [np.count_nonzero(k) for k in [d==1, d_pr[d==1.] > 0.5, d==0, d_pr[d==0.]<= 0.5]]\n",
    "    \n",
    "    acc = (tp + tn) / (len(d))\n",
    "    loss = model.evaluate(x, d , batch_size =  x.shape[0], verbose=0)\n",
    "                \n",
    "    A = ['Accuracy', 'Loss']\n",
    "    B = [acc, loss[0]]\n",
    "    \n",
    "    print('\\n','#'*10,'STATISTICS for {} data'.format(label), '#'*10, '\\n')\n",
    "    for i in range(len(A)):\n",
    "        print('{:15} {:.4f}'.format(A[i], B[i]))\n",
    "\n",
    "    return print('\\n','#'*50)\n",
    "\n",
    "def stats_reg(x = None, d = None, label='training', model = None):\n",
    "    \"\"\"\n",
    "    input :  \n",
    "             x = input\n",
    "             d = target\n",
    "             label = \"Provided text string\"\n",
    "             model = the model\n",
    "             \n",
    "    output : \n",
    "             mse = mean squared error between target and predictions\n",
    "             loss = typically mse for regression problems\n",
    "    \"\"\"\n",
    "    \n",
    "    loss = model.evaluate(x, d, batch_size = x.shape[0], verbose=0)\n",
    "    d_pr = model.predict(x, batch_size = x.shape[0], verbose=0).reshape(d.shape)\n",
    "    mse = np.mean((d - d_pr)**2)\n",
    "    \n",
    "    A = ['MSE','Loss']\n",
    "    B = [mse, loss[0]]\n",
    "    \n",
    "    print('\\n','#'*10,'STATISTICS for {} data'.format(label), '#'*10, '\\n')\n",
    "    for i in range(len(A)):\n",
    "        print('{:15} {:.10f}'.format(A[i], B[i]))\n",
    "\n",
    "    return print('\\n','#'*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: Boundary (#6)\n",
    "### CellType: Needed\n",
    "### Cell Instruction: Decision boundary\n",
    "\n",
    "This cell defines the function to plot the decision boundary for a 2D input binary MLP classifier. In short, this function defines a grid covering the input data. Each grid point is then used as an input to the trained MLP and to compute an output. If the output is close to 0.5 it is marked as the boundary. \n",
    "\n",
    "Run the cell by entering into the cell and press \"CTRL Enter\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_b(X, Y1, model ):\n",
    "    \n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    # grid stepsize\n",
    "    h = 0.025\n",
    "\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()], verbose=0)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    Z[Z>.5] = 1\n",
    "    Z[Z<= .5] = 0\n",
    "\n",
    "    Y_pr = model.predict(X, batch_size = X.shape[0], verbose=0).reshape(Y1.shape)\n",
    "  \n",
    "    Y = np.copy(Y1)\n",
    "    Y_pr[Y_pr>.5] = 1\n",
    "    Y_pr[Y_pr<= .5] = 0\n",
    "    Y[(Y!=Y_pr) & (Y==0)] = 2\n",
    "    Y[(Y!=Y_pr) & (Y==1)] = 3\n",
    "    \n",
    "    \n",
    "    plt.figure()\n",
    "    #plt.contourf(xx, yy, Z, cmap=plt.cm.PRGn, alpha = .9) \n",
    "    plt.contour(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "    \n",
    "    \n",
    "    plt.scatter(X[:, 0][Y==1], X[:, 1][Y==1], marker='+', c='k')\n",
    "    plt.scatter(X[:, 0][Y==0], X[:, 1][Y==0], marker='o', c='k')\n",
    "       \n",
    "    plt.scatter(X[:, 0][Y==3], X[:, 1][Y==3], marker = '+', c='r')   \n",
    "    plt.scatter(X[:, 0][Y==2], X[:, 1][Y==2], marker = 'o', c='r')\n",
    "    \n",
    "    \n",
    "    plt.ylabel('x2')\n",
    "    plt.xlabel('x1')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: CustomEpochStatistics (#7)\n",
    "### CellType: Needed\n",
    "### Cell Instruction: Compute alternative epoch statistics\n",
    "\n",
    "This cell defines a callback function to be executed after each completed epoch. The purpose is to be able to monitor the \"real\" loss after each epoch and not the one reported by Keras that is based on the last batch of the epoch.\n",
    "\n",
    "Run the cell by entering into the cell and press \"CTRL Enter\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpochCallback(Callback):\n",
    "    \n",
    "    def __init__(self, x_trn, y_trn):\n",
    "        self.x_trn = x_trn\n",
    "        self.y_trn = y_trn\n",
    "           \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        x_trn, y_trn = self.x_trn, self.y_trn\n",
    "        perf = self.model.evaluate(x_trn, y_trn, verbose=0, batch_size = x_trn.shape[0])\n",
    "        id1 = 'epoch_' + self.model.metrics_names[0]\n",
    "        logs[id1] = perf[0]\n",
    "        id2 = 'epoch_' + self.model.metrics_names[1]\n",
    "        logs[id2] = perf[1]\n",
    "        #print(\"Epoch {} (end): loss = {:.4f}  epoch_loss = {:.4f}\\n\".format(epoch+1, logs[\"loss\"], logs[\"epoch_loss\"]))\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "End of \"Needed\" and \"Information\" cells. Below are the cells for the actual exercise.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: Ex1 (#8)\n",
    "### CellType: Exercise\n",
    "### Cell instruction: Instruction for questions 1-4\n",
    "Questions 1-4 look at three essential parameters that controls the training process of an MLP: the *learning rate*, *batch size* and *number of epochs* (or epochs for short). By training process we mean here the minimization of the given loss function. The task is to train an MLP with four hidden nodes that can fit the *regr1* dataset. In this version of the dataset, there is no noise. Therefore, we need not specify any seed.\n",
    "\n",
    "The dataset and network have been selected so that it is possible, but not trivial, to get a good training result.\n",
    "A successful training means here when the networks has reached a loss < 0.01, and visually have fitted the data accurately. In this exercise we do not care about possible overfitting, only about the minimization of the loss function, we therefore do not have a validation dataset.\n",
    "\n",
    "#### Question 1, variations in pre-defined MLP\n",
    "For the first question you can simply run the cell below. It will load 50 samples from the *regr1* dataset (no noise added). The network has 4 hidden nodes in a single hidden layer, *tanh* activation function, linear output activation function, *stochastic gradient descent* as minimization method, MSE loss function, and a learning rate of 0.05.\n",
    "It will train for 4000 epochs using a batchsize of 50, meaning that we efficiently are using ordinary gradient descent learning. Run this cell five times. **(a) Do you see the same loss vs epoch behavior each time your run?** If not, **why?** **(b) Do you observe that training fails, i.e. do not reach low loss, during any of these five runs?** \n",
    "\n",
    "#### Question 2, vary learning rate\n",
    "You will now study what happens when you train with different learning rates. Test at least 5 different learning rates in the range from 0.001 to 0.5. For each learning rate train the network three times and record the average MSE value over these three runs. **Present your average MSE results and discuss your findings**.\n",
    "\n",
    "**Note:** You should keep the same settings as for Q1, only vary the learning rate. The learning rate is best investigated with (roughly) proportional changes rather than constant steps: For example, trying 0.5, 0.2, 0.1, 0.05, 0.02 etc. typically gives more interesting results than 0.5, 0.4, 0.3, 0.2, 0.1.\n",
    "\n",
    "#### Question 3, vary (mini)batch size\n",
    "We now (hopefully) have discovered that the learning rate influences the efficiency of the loss minimization. We will now look at what happens when we use *stochastic gradient descent*, meaning that we will have a \"batch size\" that is smaller the the size of the training data. (We now adapt to Keras language, and use the word \"batch\" where most litterature would use \"mini-batch\".) Use a fixed learning rate of 0.05, but test different batch sizes in the range 1 to 50. Train three different networks for each batch size, but this time record if the training was successful (i.e. MSE < 0.01) and approximately after how many epochs the good solution was found. **Present and discuss your findings**.\n",
    "\n",
    "**Note:** The batch size should (fairly well) divide the total data size. With a data size of 50, good batch sizes are 50, 25, 13, 10, 8, 7... Sizes that fit poorly, like 40, create a 'rest'-batch that must be handled by the Keras package somehow, and you need to carefully study Keras documentation to understand what it does.\n",
    "\n",
    "#### Question 4, select good hyper-parameters\n",
    "Find a combination of learning rate and batch size that gives a good solution within 1000 epochs. We always have to remember that two runs with identical hyper parameters (e.g. learning rate, batch size etc) will result in different final results. Your set of parameters should *most* of the times result in a good solution within 1000 epochs. **Present your best combination of learning rate and batch size, and its result**.\n",
    "\n",
    "#### Remark: What is acually plotted by Keras?\n",
    "In all of Loss/MSE per Epoch plots you have looked at so far, the values plotted are coming from \"internal\" Keras calculations. It turns out that loss and MSE values are not really calculated as true per epoch values. To speed up calculations Keras reports the loss/MSE for the last mini-batch (in the current epoch) before updating the weights. This is not the same as calculating the loss/MSE for the full training dataset at the end of the epoch. This is good to know! To illustrate this we have added what is called a CallBack function that actually computes the true loss/MSE per epoch.\n",
    "\n",
    "Add the additional argument to the fit statement:\n",
    "\n",
    "`callbacks=[EpochCallback(x_trn, d_trn)]`\n",
    "\n",
    "The modified statement should now look like this:\n",
    "\n",
    "~~~\n",
    "estimator_ex1 = model_ex1.fit(x_trn, d_trn,\n",
    "                              epochs = number_epochs,      \n",
    "                              batch_size = minibatch_size,\n",
    "                              verbose = 0, \n",
    "                              callbacks=[EpochCallback(x_trn, d_trn)])\n",
    "~~~\n",
    "\n",
    "To see the difference between what Keras produces and our own calculations of the loss/MSE per epoch use following setup:\n",
    "\n",
    "Epochs = 200, Mini-batch size = 10, Learning Rate = 0.05\n",
    "\n",
    "You should see that the curves denoted \"epoch_loss/epoch_mse\" are smoother compared to the curves denoted \"loss/mse\" that are computed by Keras. The differences between the two ways of computing the loss/mse should decrease if we make smaller updates, i.e. using a smaller learning rate, as illustrated by the follwoing setup:\n",
    "\n",
    "Epochs = 200, Mini-batch size = 10, Learning Rate = 0.01\n",
    "\n",
    "Using the CallBack functionallity to compute the \"true\" loss/mse per epoch is slower since we need to make a complete forward pass for the full training dataset at each epoch. However sometimes this is needed to find optimal minimization parameters. For the rest of the exercise we will *not* use the CallBack, to speed up the training process.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#### not used for Q1-Q4\n",
    "# seed = 0 means random, seed > 0 means fixed\n",
    "# seed = 0\n",
    "# np.random.seed(seed) if seed else None\n",
    "####\n",
    "\n",
    "# Generate training data\n",
    "x_trn, d_trn = regr1(50, 2, 0, 0.0)\n",
    "\n",
    "# Standardization of inputs\n",
    "mu, std = standard(x_trn)\n",
    "x_trn = (x_trn - mu)/std\n",
    "\n",
    "#### \n",
    "# Define the network, cost function and training settings\n",
    "INPUT1 = {'inp_dim': 1,         \n",
    "         'n_nod': [4],                   # number of nodes in hidden layer\n",
    "         'act_fun': 'tanh',              # activation functions for the hidden layer\n",
    "         'out_act_fun': 'linear',        # output activation function\n",
    "         'opt_method': 'SGD',            # minimization method\n",
    "         'cost_fun': 'mse',              # error function\n",
    "         'lr_rate': 0.05                 # learningrate\n",
    "        }    \n",
    "number_epochs = 4000\n",
    "minibatch_size = 50\n",
    "####    \n",
    "\n",
    "# Get the model\n",
    "model_ex1 = mlp(**INPUT1)\n",
    "\n",
    "# Print a summary of the model\n",
    "model_ex1.summary()\n",
    "\n",
    "# Train the model\n",
    "estimator_ex1 = model_ex1.fit(x_trn, d_trn,\n",
    "                              epochs = number_epochs,      \n",
    "                              batch_size = minibatch_size,\n",
    "                              verbose = 0)\n",
    "\n",
    "\n",
    "# Training history\n",
    "plt.figure()\n",
    "plt.ylabel('Loss / MSE')\n",
    "plt.xlabel('Epoch')\n",
    "for k in estimator_ex1.history.keys():\n",
    "    plt.plot(estimator_ex1.history[k], label = k) \n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "# Call the stat_reg to get MSE\n",
    "#pred_trn = model_ex1.predict(x_trn, verbose=0).reshape(d_trn.shape)\n",
    "stats_reg(x_trn, d_trn, 'Training', model_ex1)\n",
    "\n",
    "# Plot the result\n",
    "d_pred = model_ex1.predict(x_trn, verbose=0)\n",
    "plt.figure()\n",
    "plt.ylabel('Prediction / Target')\n",
    "plt.xlabel('Input')\n",
    "plt.scatter(x_trn, d_trn, label='Target')\n",
    "plt.scatter(x_trn, d_pred, label='Prediction')\n",
    "plt.title('Prediction vs Target')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: Ex2 (#9)\n",
    "### CellType: Exercise\n",
    "### Cell instruction: Instruction for questions 5-6\n",
    "The amount of weights in the network can also influence how long time we need to train, and of course if the problem itself is complex or not. The following two questions will highlight this.\n",
    "\n",
    "#### Question 5, vary epochs\n",
    "The example below will load a slightly more complex *regr1* problem (an additional quarter of a period). We will use 10 hidden nodes for this problem. Use your optimal set of learning rate and batch size as found in Q4 and train the network below. **Compare the number of epochs needed to reach a good solution with that of Q4**. Note, you may need to vary the number of epochs a lot! If you cannot find a good solution in a reasonable number of epochs, you can \"revert\" the problem: optimize learning rate and batch size for Q5, and the see how those hyper-parameters perform on Q4.\n",
    "\n",
    "#### Question 6, vary network size and other hyper-parameters\n",
    "Use the following line to load the *regr1* data set:\n",
    "\n",
    "`x_trn, d_trn = regr1(75, 5, 1, 0.0)`\n",
    "\n",
    "This will create an even more challenging regression task that may need an even larger network. Your task is to find a set of hyper-parameters (learning rate, batch size, epochs, 'size of the network') that result in a good solution. You can use more than one hidden layer for this task (if you want). To create many hidden layers, add many numbers to the n_nod list, for example: `'n_nod': [10,5,5]`. **Present your set of good hyper-parameters and the result**. \n",
    "\n",
    "**Note:** If you cannot solve this task in *reasonable* time, present your best attempt!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# seed = 0 means random, seed > 0 means fixed\n",
    "seed = 0\n",
    "np.random.seed(seed) if seed else None\n",
    "\n",
    "# Generate training data\n",
    "# For Q5:\n",
    "x_trn, d_trn = regr1(50, 2.5, 0, 0.0)\n",
    "\n",
    "# For Q6:\n",
    "#x_trn, d_trn = regr1(75, 5, 1, 0.0)\n",
    "\n",
    "# Standardization of inputs\n",
    "mu, std = standard(x_trn)\n",
    "x_trn = (x_trn - mu)/std\n",
    "\n",
    "#### \n",
    "# Define the network, cost function and training settings\n",
    "INPUT2 = {'inp_dim': 1,         \n",
    "         'n_nod': [10],                  # number of nodes in hidden layer\n",
    "         'act_fun': 'tanh',              # activation functions for the hidden layer\n",
    "         'out_act_fun': 'linear',        # output activation function\n",
    "         'opt_method': 'SGD',           # minimization method\n",
    "         'cost_fun': 'mse',              # error function\n",
    "         'lr_rate': 0.05                 # learningrate\n",
    "        }        \n",
    "number_epochs = 4000\n",
    "minibatch_size = 50\n",
    "####\n",
    "\n",
    "# Get the model\n",
    "model_ex2 = mlp(**INPUT2)\n",
    "\n",
    "# Print a summary of the model\n",
    "model_ex2.summary()\n",
    "\n",
    "# Train the model\n",
    "estimator_ex2 = model_ex2.fit(x_trn, d_trn,\n",
    "                              epochs = number_epochs,      \n",
    "                              batch_size = minibatch_size,\n",
    "                              shuffle=True,\n",
    "                              verbose = 0)\n",
    "\n",
    "\n",
    "# Training history\n",
    "plt.figure()\n",
    "plt.ylabel('Loss / MSE')\n",
    "plt.xlabel('Epoch')\n",
    "for k in estimator_ex2.history.keys():\n",
    "    plt.plot(estimator_ex2.history[k], label = k) \n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "# Call the stat_reg to get MSE and correlation coefficiant for the scatter plot\n",
    "stats_reg(x_trn, d_trn, 'training', model_ex2)\n",
    "\n",
    "# Plot the result\n",
    "d_pred = model_ex2.predict(x_trn, verbose=0).reshape(d_trn.shape)\n",
    "plt.figure()\n",
    "plt.ylabel('Prediction / Target')\n",
    "plt.xlabel('Input')\n",
    "plt.scatter(x_trn, d_trn, label='Target')\n",
    "plt.scatter(x_trn, d_pred, label='Prediction')\n",
    "plt.title('Prediction vs Target')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: Ex3 (#10)\n",
    "### CellType: Exercise\n",
    "### Cell instruction: Instructions for questions 7-8\n",
    "We will now look at the classification problem defined by the *syn1* dataset.\n",
    "The cell below defines a single hidden node MLP. With this network you can only implement a linear decision boundary. Run the cell below to look at the resulting boundary that the MLP learns. The training accuracy is around 87-93%, because the data is generated randomly each time you run the code. \n",
    "\n",
    "#### Question 7, optimize hyper-parameters for classification\n",
    "Your task is now to reach a larger accuracy by fitting a model with more hidden nodes (and possibly more hidden layers). \n",
    "Your aim is to reach a training accuracy > 95%. To do that you need to tune the learning rate, batch size, epochs and the size of your MLP. **Present your set of hyper parameters that reach > 95% accuracy**\n",
    "\n",
    "**Note**: To always generate exactly the same dataset each time you run the code you can set the *seed* to a value > 0. \n",
    "\n",
    "#### Question 8, change learning algorithm\n",
    "We have so far only used stochastic gradient descent (SGD), but we know that there exists modifications of SGD that are very popular, e.g. Adam. \n",
    "**Try the Adam optimizer for Q7, and compare (qualitatively) the results and the number of epochs needed to get them.**\n",
    "\n",
    "The interpretation of the learning rate differs a bit between SGD and Adam. Since your learning rate was optimized for SGD in Q7, you could consider optimizing it again for Adam, before you compare SGD with Adam. **Present changes you needed to make to improve the results of the Adam optimizer, if any.**\n",
    "\n",
    "\n",
    "**Info**: Adam has two extra parameters. The way we call the Adam optimizer, they will be kept at their default values *beta1* = 0.9 and *beta2* = 0.999. \n",
    "\n",
    "#### Bonus tasks\n",
    "The bonus tasks are provided if you have extra time and want to continue to explore methods that can further enhance the minimization of the loss function. **These tasks are not required for the course and do not influence any grading**. \n",
    "\n",
    "The tasks listed below also mean that you have to change the code in code cell *MLP* (#2). There will be links to appropriate documentation below.\n",
    "\n",
    "* Go back to Q7 and add use a momentum add-on to SGD. **Does momentum help?** (See documentation [here](https://keras.io/api/optimizers/sgd/))\n",
    "* It is common to also introduce a mechanism that can lower the learning rate as we train. If we are using stochastic gradient descent the mini-batch gradients will never be zero, meaning that we will always make some small weight updates. Keras have methods that can lower the learning rate as we train (see [here](https://keras.io/api/optimizers/learning_rate_schedules/exponential_decay/)). Again go back to Q7 and now use an exponential decaying learning rate. **Does it help?**  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# seed = 0 means random, seed > 0 means fixed\n",
    "seed = 10\n",
    "np.random.seed(seed) if seed else None\n",
    "    \n",
    "# Generate training data\n",
    "x_trn, d_trn = syn2(100)\n",
    "\n",
    "# General standardization of input data \n",
    "mu, std = standard(x_trn)\n",
    "x_trn = (x_trn - mu)/std\n",
    "\n",
    "#### \n",
    "# Define the network, cost function and training settings\n",
    "INPUT3 = {'inp_dim': x_trn.shape[1],         \n",
    "         'n_nod': [1],                      # number of nodes in hidden layer\n",
    "         'act_fun': 'tanh',                 # activation functions for the hidden layer\n",
    "         'out_act_fun': 'sigmoid',          # output activation function\n",
    "         'opt_method': 'SGD',               # minimization method\n",
    "         'cost_fun': 'binary_crossentropy', # error function\n",
    "         'lr_rate': 0.1,                    # learning rate\n",
    "         'metric': 'accuracy'               # metric for evaluation\n",
    "        } \n",
    "number_epochs = 500\n",
    "minibatch_size = 100\n",
    "####\n",
    "\n",
    "# Get the model\n",
    "model_ex3 = mlp(**INPUT3)\n",
    "\n",
    "# Print a summary of the model\n",
    "model_ex3.summary()\n",
    "\n",
    "# Train the model\n",
    "estimator_ex3= model_ex3.fit(x_trn, d_trn,\n",
    "                      epochs = number_epochs,                \n",
    "                      batch_size = minibatch_size,                   \n",
    "                      verbose = 0)\n",
    "\n",
    "# Call the stats function to print out statistics for the training\n",
    "stats_class(x_trn, d_trn, 'Training', model_ex3)\n",
    "\n",
    "# Training history\n",
    "plt.figure()\n",
    "plt.ylabel('Loss / Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "for k in estimator_ex3.history.keys():\n",
    "    plt.plot(estimator_ex3.history[k], label = k) \n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "# Show the decision boundary\n",
    "decision_b(x_trn, d_trn, model_ex3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The report!\n",
    "\n",
    "We have added intructions inside this report template. As you write your report, remove the instructions.\n",
    "\n",
    "## Name\n",
    "\n",
    "## Introduction\n",
    "A few sentences about the overall theme of the exercise.\n",
    "\n",
    "## Answers to questions\n",
    "Provide enough information to clarify the meaning of your answers, so that they can be understood by someone who does not scroll up and read the entire instruction.\n",
    "\n",
    "The questions are repeated here, for clarity of what is demanded. If it does not fit your style to quote them verbatim, change the format. \n",
    "\n",
    "#### Question 1, variations in pre-defined MLP\n",
    "(a) Do you see the same loss vs epoch behavior each time your run? If not, why?\n",
    "\n",
    "(b) Do you observe that training fails, i.e. do not reach low loss, during any of these five runs? \n",
    "\n",
    "#### Question 2, vary learning rate\n",
    "Present your average MSE results and discuss your findings.\n",
    "\n",
    "#### Question 3, vary (mini)batch size\n",
    "Present and discuss your findings.\n",
    "\n",
    "#### Question 4, select good hyper-parameters\n",
    "Present your best combination of learning rate and batch size, and its result.\n",
    "\n",
    "#### Question 5, vary epochs\n",
    "Compare the number of epochs needed to reach a good solution with that of Q4. If you cannot find a good solution in a reasonable number of epochs, you can \"revert\" the problem: optimize learning rate and batch size for Q5, and the see how those hyper-parameters perform on Q4.\n",
    "\n",
    "#### Question 6, vary network size and other hyper-parameters\n",
    "Present your set of good hyper-parameters and the result. \n",
    "\n",
    "Note: If you cannot solve this task in *reasonable* time, present your best attempt!\n",
    "\n",
    "#### Question 7, optimize hyper-parameters for classification\n",
    "Present your set of hyper-parameters that reach > 95% accuracy\n",
    "\n",
    "#### Question 8, change learning algorithm\n",
    "Try the Adam optimizer for Q7, and compare (qualitatively) the results and the number of epochs needed to get them. Present changes you needed to make to improve the results of the Adam optimizer, if any.\n",
    "\n",
    "#### Bonus tasks (if you feel inspired)\n",
    "\n",
    "## Summary\n",
    "Connect the summary to your introduction, to provide a brief overview of your findings.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "530px",
    "width": "356.167px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
